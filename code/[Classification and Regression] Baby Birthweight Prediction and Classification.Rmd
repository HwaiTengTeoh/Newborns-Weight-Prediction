---
title: 'Predicting Birth Weight of Baby'
author:
  - Law Chuan Liang (S2003493)
  - Teoh Hwai Teng (S2016411)
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
    rmdformats::readthedown:
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

```{r required_package, include=FALSE}
library(readr)
library(tidyverse)
library(ggplot2)
library(caret)
library(class)
library(nnet)
library(pROC)
library(psych)
library(xgboost)
```

# Introduction 

Birth weight is the weight of baby taken just after he or she is born, and a baby birth weight is a strong indicator of maternal and new-born health and nutrition. According to World Health Organization (WHO), low birth weight is defined as the birthweight less than 2,500 grams or 5.5 pounds regardless of gestational age. The normal weight range for new-born baby should exceed 2,500 grams (5.5 pounds) but less than 4,000 grams (8.8 pounds). 

Low birth weight is an outcome that has been of concern because infant mortality rates and birth defect rates are very high for low birth weight babies. Low birth weight can occur in premature babies who are delivered at a gestational age less than 37 weeks of pregnancy (the normal length of a pregnancy is 40 weeks) or in babies who are born at the regular time but are under weight. Baby with greater than the normal weight range is considered large and would increase risk and difficulties during delivery. 


## Motivation of study

It would be so important for the doctor to be able to predict the birth weight instead of solely relying on ultrasound results, so they can take different measures in advance and minimize the risk during delivery. Though ultrasound can help for such prediction, but data scientist could also detect this in advance with given data. 


# Problem statement

Hereâ€™re some interesting questions arise from this scenario that we are considering for the study:

1.	What is the weight of new-born baby given with some information during gestation period?
2.	Is the baby too small (underweight), normal or large (overweight) in size? 
3.	What are the factors associated with giving birth to a low birth weight and high birth weight baby?


# Objective of study

The main goal of this project is to obtain prediction for the birthweight of the baby and whether a baby is underweight, overweight or has normal weight given information about the mother's historical health, habits, gestational period and age. Thus, several models are built to predict the birthweight of the baby and identify if the baby have low, normal or high birth weight using classification method.


# Overview of data

The Dataset used is collected by North Carolina State Center for Health Statistics and we acknowledge The State Center for Health Statistics (SCHS) and the Howard W. Odum Institute for Research in Social Science at UNC at Chapel Hill as the source of data. 

## Import data

```{r load_tidyverse, eval=FALSE}
###################
# Load R packages #
###################
library(readr)
library(tidyverse)
```

```{r read_data, collapse=TRUE}
###################################
# Import data: Baby's birthweight #
###################################

birthweight = readr::read_csv(file="2008_births.csv")
```

The data was collected in 2008 with `r nrow(birthweight)` rows with `r ncol(birthweight)` columns. The target variables for our study is the birth weight of infant which can be represented from columns namely `BPOUND` and `BOUNCE` which denote the birthweight in pounds and ounce separately. 
```{r max.height='200px'}
str(birthweight)
```

```{r collapse=TRUE}
dim(birthweight)
nrow(birthweight)
ncol(birthweight)
```

There are many features and columns which are not relevant to our scope of objectives. Features about post birth, birth details such as date are all removed to keep simplicity for pre-processing of data later.

```{r keep_col}
keep_col = c('SEX', 'MARITAL','FAGE', 'GAINED', 'VISITS', 'MAGE', 
            'FEDUC', 'MEDUC', 'TOTALP', 'BDEAD', 'TERMS',  
            #'RACEMOM', 'RACEDAD', 'HISPMOM', 'HISPDAD', 'LOUTCOME',
            'PLURAL','WEEKS','CIGNUM', 'DRINKNUM', 'ANEMIA', 'BPOUND','BOUNCE',
            'CARDIAC', 'ACLUNG', 'DIABETES', 'HERPES', 'HYDRAM', 
            'HEMOGLOB', 'HYPERCH', 'HYPERPR', 'ECLAMP', 'CERVIX',
            'PINFANT','PRETERM','RENAL','RHSEN','UTERINE')
```

```{r select_col}
birthweight_clean = birthweight %>%  
                    select(all_of(keep_col))
```

After removal of the unrelevant features, the dataset is trimmed to have only `r ncol(birthweight_clean)` columns and `r nrow(birthweight_clean)` rows of observations.

```{r collapse=TRUE}
dim(birthweight_clean)
nrow(birthweight_clean)
ncol(birthweight_clean)
```


Let's dive into the dataset to get an overview. The description of each attributes in the dataset is attached under Appendix section.  

```{r max.height='200px'}
str(birthweight_clean)
```

We have both qualitative and quantitative variables in the dataset. Determined factors for baby birthweight such as mother's weight gained, parents age during gestational period, number of gestational weeks etc will be used to build the models.  

Most qualitative variables are in binary form which describe mother's health condition on whether she suffer from anaemia, cardiac disease,lung disease, diabetes, genital herpes, Oligohydramnios, hemoglobinopathy, hypertension, pregnancy hypertension, Eclampsia, incompetent cervix and renal disease. Other information such as whether the mother had preterm infant or infant exceed 4,000g and experience uterine bleeding during gestation period are also included in the dataset.  

```{r max.height='200px'}
summary(birthweight_clean)
```

Surprisingly, there is no missing values found in the dataset. However, when we notice carefully from the summary statistics for each column. Qualitative variable denoted in numeric have maximum values of 9 which seems odd. Similar patterns are found in quantitative variables with maximum values recorded as 98 or 99 which are not describing the attributes accurately. These values are actually recoded from the missing values during recording of the data.

```{r}
sum(!complete.cases(birthweight_clean))
```

# Data pre-processing
## Dealing Missing data
Observations with missing data are omitted from the dataset as there are just less than 10% of them are missing.

```{r}
qualitative_col =c('SEX', 'MARITAL','ANEMIA',
                  'CARDIAC', 'ACLUNG', 'DIABETES', 'HERPES', 'HYDRAM', 
                  'HEMOGLOB', 'HYPERCH', 'HYPERPR', 'ECLAMP', 'CERVIX',
                  'PINFANT','PRETERM','RENAL','RHSEN','UTERINE')

birthweight_clean = birthweight_clean %>%
      filter_at(all_of(qualitative_col), all_vars(. != 9)) %>%
      filter(!rowSums(.==99)) %>%
      filter(!rowSums(.==98))
```

## Transformation of data
Both `BPOUND` and `BOUNCE` columns that record the pounds and ounces of the weight are merged to create new column `BWEIGHT` which give the birth weight in decimal. Conversion of ounce to pound is computed by multiplying `BOUNCE` column with 0.0625. 

We also create new column `BWEIGHT_COND` to record the groupings of baby birthweight. Babies with low birthweight are those with less than 5.5 pounds. Large baby with birthweight exceeds 8.8 pounds are grouped as overweight category. Other than that, the baby are in the normal range of birthweight. This will be used for building the classification model which will be discussed shortly.

```{r}
birthweight_clean = 
  birthweight_clean %>% 
  mutate(BWEIGHT=BPOUND+(0.0625*BOUNCE)) %>%
  mutate(BWEIGHT_COND=case_when(
                      BWEIGHT <5.5 ~ 0,
                      BWEIGHT >=5.5 & BWEIGHT <8.8 ~ 1,
                      BWEIGHT >=8.8 ~ 2)
         )%>%
  select(-c("BPOUND","BOUNCE")) 
```

## Dummy coding categorical features

We have some nominal variables such as `SEX`, `MARITAL` and the newly created `BWEIGHT_COND`. We are performing dummy coding which creates a set of binary (one-zero) variables that represent each category before we can use to fit and evaluate a model.

`BWEIGHT_COND` is currently represented with numbers of 0, 1, 2 for 'Underweight', 'Normal' and 'Overwieght'. It may be advisable to convert it to a factor to allow the category to have unique impact on the outcome.

```{r}
birthweight_clean = 
  birthweight_clean %>% 
  mutate(SEX_dummy_Boy = ifelse(SEX==1,1,0),
         SEX_dummy_Girl = ifelse(SEX==2,1,0),
         STATUS_dummy_Married=ifelse(MARITAL==1,1,0),
         STATUS_dummy_Unmarried=ifelse(MARITAL==2,1,0),
         BWEIGHT_dummy_under=ifelse(BWEIGHT_COND==0,1,0),
         BWEIGHT_dummy_normal=ifelse(BWEIGHT_COND==1,1,0),
         BWEIGHT_dummy_over=ifelse(BWEIGHT_COND==2,1,0)
      ) %>% 
  mutate_at(vars(BWEIGHT_COND ),factor,levels = c(0,1,2),labels=c("Underweight","Normal","Overweight")) %>%
  mutate_at(vars(BWEIGHT_COND ),relevel,ref="Normal")

```


# Exploratory Data Analysis

Before modelling, let's explore each attribute to uncover the structure, patterns, and relationships existing in our data.

## Targeted variables

Most babies are reported to have weights of range 7 pounds to 8 pounds as we can see the peak of histogram lies in between this range. The dataset has the highest frequency for babies with normal weight which is a good sign as the baby are in healthy condition. 

```{r collapse=TRUE}
library(gridExtra)

plot1=birthweight_clean %>% 
        ggplot(aes(x=BWEIGHT)) + 
        geom_histogram(fill="#9f2042")  +
        geom_density(alpha=.2, fill="#FF6666") +
        xlab("Birthweight (pounds)") +
        ylab("Frequency") +
        scale_x_continuous(breaks=seq(0,10,1))
        ggtitle("Histogram of Birthweight")

plot2=birthweight_clean %>% 
        ggplot(aes(x=BWEIGHT_COND)) + 
        geom_bar(fill=c("#9f2042","#211103","#1F1D1D")) +
        xlab("Classes") +
        ylab("Frequency") +
        ggtitle("Birthweight classes")
```

```{r}
grid.arrange(plot1, plot2,ncol=2, nrow = 1)
```

## Bivariate analysis
The relationship of the birthweight and other features will be discussed shortly. 
```{r}
plot_density = function(cond,true_lab,false_lab,title){
  birthweight_clean %>% 
    mutate(Status=ifelse(cond,true_lab,false_lab)) %>%
    ggplot(aes(x=BWEIGHT,color=Status)) +
    geom_density(size=1) +
    ylab("Density") +
    xlab("Birthweight") +
    ggtitle(title) +
    scale_color_manual(values=c("#211103","#9f2042")) 
}

# Smoker vs Non-Smoker
plot3 = plot_density(birthweight_clean$CIGNUM>0,
                     "Smoker","Non-smoker",
                     "Status of smoking")

plot4 = plot_density(birthweight_clean$DRINKNUM>1,
                     "Drink","Not drinking",
                     "Status of taking/ not taking alcohol")  

# has/had previus preterm/small infant
plot5 = plot_density(birthweight_clean$PRETERM==1,
                     "Yes","No",
                     "Has/had previous preterm/small infant") 

# had/had previous infant 4000+ grams
plot6 = plot_density(birthweight_clean$PINFANT==1,
                     "Yes","No",
                     "Has/had previous infant with >5.5 pounds")  

# Mother has/had incompetent cervix
plot7 = plot_density(birthweight_clean$CERVIX==1,
                     "Yes","No",
                     "Has/had incompetent Cervix")  


# Mother has/had diabetes
plot8 = plot_density(birthweight_clean$DIABETES==1,
                     "Yes","No",
                     "Has/had diabetes")

# Mother has/had uterine bleeding
plot9 = plot_density(birthweight_clean$UTERINE==1,
                     "Yes","No",
                     "Has/had uterine bleeding")

# Mother has/had Oligohydramnios
plot10 = plot_density(birthweight_clean$HYDRAM==1,
                     "Yes","No",
                     "Has/had Oligohydramnios")

# Mother has/had chronic hypertension
plot11 = plot_density(birthweight_clean$HYPERCH==1,
                     "Yes","No",
                     "Has/had chronic hypertension")

# Mother has/had pregnancy hypertension
plot12 = plot_density(birthweight_clean$HYPERPR==1,
                      "Yes","No",
                      "Has/had pregnancy hypertension")

# Early delivery vs Normal delivery
plot13 = plot_density(birthweight_clean$WEEKS<37,
                     "Early delivery","Normal delivery",
                     "Early delivery vs Normal delivery")


# Early delivery vs Normal delivery
plot14 = plot_density(birthweight_clean$BDEAD>0,
                     "Yes","Never/No",
                     "Has/had children born alive now dead")
```

Based on the density plots, we observed that:

- Mother who smokes during pregnancy tends to have low birth weight babies.
- Alcoholic seems does not bring significant effect to the birthweight of baby but this is just based on the observation from this data as we require more scientific justification for such claims. 
- Mother who experienced preterm delivery are likely to give birth of low weight baby. In contrast, mothers who experienced in giving birth of larger baby have higher possibility to have infants with greater birthweight. 

Condition of mother's health can be determined factors of baby birthweight. We observed that mother who suffer from

- uterine bleeding
- ologohydramnios
- hypertensions

has higher probability with low birthweight baby.

```{r max.height='250px'}
grid.arrange(plot3,plot4,plot5,plot6,plot7,plot8,ncol=2)
```

```{r max.height='250px'}
grid.arrange(plot9,plot10,plot11,plot12,plot13,plot14,ncol=2)
```

## Correlation 

```{r}
library(reshape2)

quantitative_col =c('FAGE', 'GAINED', 'VISITS', 'MAGE', 'BWEIGHT','PLURAL',
                    'FEDUC', 'MEDUC', 'TOTALP', 'BDEAD', 'TERMS', 
                    'WEEKS','CIGNUM', 'DRINKNUM')

# Correlation matrix for all quantitative variables
corr_matrix <- round(cor(birthweight_clean[,quantitative_col]),2)

# Get upper triangle of the correlation matrix
get_upper_tri <- function(corr_matrix){
  corr_matrix[lower.tri(corr_matrix)]<- NA
  return(corr_matrix)
}

upper_tri <- get_upper_tri(corr_matrix)
```

Matrix of correlation with upper triangle is displayed.
```{r max.height='200px'}
upper_tri
```

From the heatmap, we observed that weeks of gestation is positively correlated with birthweight. The longer the week of gestation, the higher the weight of the baby which potentially lead to baby to grow bigger in size during delivery if the gestation weeks are longer than usual. 

Other factors such as number of cigarettes per day has negative association with baby birthweight. Mother who smokes oftenly tends to have low birth weight baby.

Other interesting facts observed:
- Mother who has twins, triplets or even quadruplets has shorter gestational weeks than single baby and babies are ligthter.
- The more weight gained during pregnancy, the baby birthweight is heavier
- Mother who vists hospital more often has baby with higher birthweight

```{r}
# Heatmap
melted_cormat <- melt(upper_tri, na.rm = TRUE)

ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "#211103", high = "#9f2042", mid = "white", 
                       midpoint = 0, limit =c(min(melt(corr_matrix)$value),max(melt(corr_matrix)$value)), space = "Lab", 
                       name="Pearson\nCorrelation") +

  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 10, hjust = 1))+
  xlab("")+ylab("")+ggtitle("Correlation Heat Map")+
  coord_fixed()
```

# Modelling
## Modelling: Classification

### k Nearest Neighbors (kNN)

k-Nearest Neighbors (kNN) uses the principle of nearest neighbors to classify unlabeled examples by searching dataset for the historic observation with most similar to the newly-observed one. As the kNN algorithm literally "learns by example" it is suitably used for classification to indentify if weights grouping for baby when birth.

#### Data processing for kNN

Since kNN assumes numeric data, categorical columns should be modified in order for kNN to measure it in numeric way. Alternative solution here is by creating dummy variable which has been completed in previous steps. 

Before applying kNN to a classification task, it is common practice to rescale the data using a technique like min-max normalization to ensure all data elements may contribute equal shares to distance.

```{r}
# Data preparation for kNN
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# Columns require normalization
birthweight_class= birthweight_clean %>%
  mutate_at(quantitative_col,normalize) %>%
  select(-contains("BWEIGHT_dummy"),-c("SEX","MARITAL","BWEIGHT"))


```

### Train/Test split for Classification

We are spliting `birthweight_class` into a training set `birthweight_class_train` (75% of the data) and a test set `birthweight_class_test` (25% of the data). A column of uniform random numbers between 0 and 1, using the function runif() is generated to get random subset of data in 75% and 25% split.

```{r collapse=TRUE}
# Use nrow to get the number of rows
(N <- nrow(birthweight_class))

# Calculate how many rows 75% of N should be and print it
(round(0.75*N))

# Create the vector of N uniform random variables: split
set.seed(123)
split <- runif(N)

# Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)
birthweight_class_train <- birthweight_class[split<0.75,]
birthweight_class_test <- birthweight_class[split>=0.75,]

# Use nrow() to birthweight_class_train and birthweight_class_test
nrow(birthweight_class_train)
nrow(birthweight_class_test)
```

```{r}
# Target variable indexes
target_index = grep("BWEIGHT_COND", colnames(birthweight_class))

# Class of train data
COND = birthweight_class_train$BWEIGHT_COND
```

There is no universal rule for selection of k. Some suggest a rule of thumb starting with k equal to the square root of the number of observations in the training data which is `r round(sqrt(nrow(birthweight_class_train)))`.

```{r eval=FALSE, include=FALSE}
#ctrl <- trainControl(method="repeatedcv",repeats = 3)
#knnFit <- train(Species ~ ., data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"),tuneLength = 20)
#knnFit
```

```{r}
# When k=65 
KNN_pred_65 <- knn(train = birthweight_class_train[-target_index], 
                    test = birthweight_class_test[-target_index], 
                    cl = COND,
                    k=65)
```

```{r}
# When k=100
KNN_pred_100 <- knn(train = birthweight_class_train[-target_index], 
                    test = birthweight_class_test[-target_index], 
                    cl = COND,
                    k=100)
```

```{r}
# When k=279 (square root of total observations)
KNN_pred_279 <- knn(train = birthweight_class_train[-target_index], 
                test = birthweight_class_test[-target_index], 
                cl = COND,
                k=279)
```

```{r collapse=TRUE}
# Create a confusion matrix of the predicted versus actual values
COND_actual <- birthweight_class_test$BWEIGHT_COND

# Accuracy rate
k_65=mean(COND_actual==KNN_pred_65)
k_100=mean(COND_actual==KNN_pred_100)
k_279=mean(COND_actual==KNN_pred_279)

k_result = rbind(k_65,k_100,k_279)
k_result
```

When k=65, we are considering 65 neighbors to consider when making the classification. 
```{r max.height='180px'}
confusionMatrix(COND_actual,KNN_pred_65)
```

When k=100, we are considering 100 neighbors to consider when making the classification. 
```{r max.height='180px'}
confusionMatrix(COND_actual,KNN_pred_100)
```

When k=279, we are considering 279 neighbors to consider when making the classification. 
```{r max.height='180px'}
confusionMatrix(COND_actual,KNN_pred_279)
```

With larger k values, we could get higher rate of accuracy in detecting normal baby weight. However, the model is not performing well in detecting the underweight and overweight babies. To get the balance, we chose smaller k values of 65, with greater accuracy of `r k_65` in overall in detecting the labelled classes. 


### Multinomial Logistic regression 
Another popular classification model is Multinomial Logistic regression which is an extension of binomial logistic regression which allows us to predict a categorical dependent variable which has more than two levels.

```{r collapse=TRUE}
# Fit the model
fmla=as.formula(BWEIGHT_COND ~ .)

log_reg_model = nnet::multinom(fmla, data = birthweight_class_train)
``` 

```{r max.height='180px'}
# Summarize the model
summary(log_reg_model )
```

After fitting the model, we predict the labelled category of `birthweight_class_train` dataset and check for its accuracy from the confusion matrix. 

```{r collapse=TRUE}
# Make predictions
predict_log_reg <- log_reg_model  %>% predict(birthweight_class_test[-target_index])

# Model accuracy
COND_actual <- birthweight_class_test$BWEIGHT_COND

logreg = mean(COND_actual==predict_log_reg)
```

```{r max.height='200px'}
confusionMatrix(COND_actual,predict_log_reg)
```

This model performs quite well too as it get an accuracy of `r logreg*100`%.

### Evaluation of classification models
By comparing the accuracy rate from both KNN and multinomial regression models, the later results with higer accuracy. Thus, in terms of classification, multinomial regression models will be selected.

```{r collapse=TRUE}
acc = cbind(k_65,k_100,k_279,logreg)
colnames(acc) = c("KNN (k=65)","KNN (k=100)","KNN (k=279)","Multinomial reg")
acc
```


## Modelling: Regression

```{r}
birthweight_reg= birthweight_clean %>%
  select(-contains("BWEIGHT_dummy"),-c("SEX","MARITAL","BWEIGHT_COND"))
```

### Latent Variable Analysis

Principal Component Analysis (PCA) is a tool for data reduction. There are several ways PCA contributes to the model development process. Other than using the output of the PCA directly into machine learning models, it can be used to identify latent variables in a dataset, by looking for how individual observed variables hang together.

```{r pca, warning=FALSE}
bw_reg_pca<-principal(select(birthweight_reg[split<0.75,],-BWEIGHT),nfactors = 15)



plot15=ggplot(data = data.frame(y=bw_reg_pca$Vaccounted[2,],x=1:15),aes(x=x,weight=y))+
  geom_bar(fill="#9f2042")+
  xlab("Component")+
  ylab("Prop. Variation")+
  ggtitle("Skree Plot")+
  scale_x_continuous(breaks=seq(0,15,1))

plot15

```
Based on the Skree Plot, 15 principle components manage to explains 60% of the variance in the data.

```{r max.height='200px'}
bw_reg_pca$loadings

```
Several observations from the PCA:

- Either one of the dummy variables for genders and married status can be removed.
- The father's age and mother's age are often falls under the same component. Father's age will be removed.
- The father's years of education and mother's years of education are often falls under the same component. Father's years of education will be removed.
- The "Number of other terminations" and "Total pregnancies" often falls under the same component. A new feature will be created by dividing "Number of other terminations" with "Total pregnancies".
- The "Number of prenatal visits" and "Completed weeks of gestation" often falls under the same component. A new feature will be created by dividing "Number of prenatal visits" with "Completed weeks of gestation".


```{r new_col}

func_col_clean<-function(dat){
  
  dat<-mutate(dat
              #,parent_edu=FEDUC*MEDUC
              ,TERMS_perc=TERMS/TOTALP
              ,VISITS_per_WEEKS=VISITS/WEEKS
              )
  
  dat<-select(dat,-FAGE,-STATUS_dummy_Unmarried
              ,-STATUS_dummy_Married
              ,-FEDUC
              #,-MEDUC
              ,-SEX_dummy_Girl
              ,-TOTALP)
  return(dat)
}

birthweight_reg2<-func_col_clean(birthweight_reg)

```

### Feature Selection

The Variable Importance is extracted using the VarImp function from caret package. Factor with high variance of importance usually have high predictive power on the target variable.

```{r var_imp, results = 'hold',warning=FALSE}

control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(BWEIGHT~., data=birthweight_reg2[split<0.75,], method="lm", preProcess="scale"
               , trControl=control)
importance <- varImp(model, scale=T)
print(importance)
plot(importance)

```
Based on the Variable Importance diagram, `PLURAL`, `WEEKS`, `GAINED`, `MAGE`, `CIGNUM`, `HYPERPR`, `PINFANT`, `DIABETES` and `HYDRAM` are selected into the machine learning model.

### Train/Test split for Regression

```{r}

quantitative_col_reg<-c("PLURAL","WEEKS","GAINED","MAGE"                         
                        ,"CIGNUM","HYPERPR","PINFANT","DIABETES","HYDRAM","BWEIGHT")

y_max=max(birthweight_reg2$BWEIGHT)
y_min=min(birthweight_reg2$BWEIGHT)

birthweight_reg3<-mutate_at(birthweight_reg2,quantitative_col_reg,normalize)
birthweight_reg_train<-birthweight_reg3[split<0.75,]
birthweight_reg_test<-birthweight_reg3[split>0.75,]

```

### Linear Regression

Linear regression is one of the basic regression model for continuous value prediction. It formulate a linear equations by assigning weights on the independent variable via maximum likelihood estimation.

```{r warning=FALSE}
f<-as.formula(paste0("BWEIGHT~",paste0(c(quantitative_col_reg,"SEX_dummy_Boy"),collapse = "+")))

bw_lm<-lm(f,data=birthweight_reg_train)

birthweight_reg_train2<-cbind.data.frame(birthweight_reg_train,BWEIGHT_hat=predict(bw_lm))
birthweight_reg_test2<-cbind.data.frame(birthweight_reg_test
                                        ,BWEIGHT_hat=predict(bw_lm
                                                             ,newdata = birthweight_reg_test))

RMSE_lm_train<-sqrt(mean((birthweight_reg_train2$BWEIGHT_hat*(y_max-y_min)+y_min-birthweight_reg_train2$BWEIGHT*(y_max-y_min)+y_min)^2))

RMSE_lm_test<-sqrt(mean((birthweight_reg_test2$BWEIGHT_hat*(y_max-y_min)+y_min-birthweight_reg_test2$BWEIGHT*(y_max-y_min)+y_min)^2))
```

```{r max.height='180px'}
summary(bw_lm)
```
Although the p-value of all the variables and final model are very small (i.e. < 2.2e-16), the R-squared looks quite low, i.e. only 40% of the output variable can be explained by the input variables. 

To investigate further, let's look at the Residuals vs Fitted graph and the QQ plot of the linear regression.
```{r}
plot(bw_lm)
```

Linear regression assumes the data is Homoscedastic (i.e. the residual value is consistent for all observations) and normally distributed. However, the Residual vs Fitted values shows that the residual of the dataset is heteroskedastic and the QQ plot reveals that the data has more extreme values than would be expected for a normal distribution.

Usually, the problem might be solved by transforming the target variables or independent variables. Instead of trying out numerous transformation method, we could attempt to develop another machine learning model.

### XGBoost

XGBoost is one of the machine learning model that could handle non-linear and non-normal distributed data.

```{r warning=FALSE, max.height='180px'}

quantitative_col_xbg<-c(quantitative_col_reg[quantitative_col_reg!="BWEIGHT"],"SEX_dummy_Boy")

bw_xgb<-xgboost(data=as.matrix(birthweight_reg_train[,quantitative_col_xbg])
                        ,label = birthweight_reg_train$BWEIGHT
                        , max.depth = 2, nrounds = 50)


birthweight_reg_train2<-cbind.data.frame(birthweight_reg_train2
                                         ,BWEIGHT_hat_xgb=predict(bw_xgb,newdata = as.matrix(birthweight_reg_train[,quantitative_col_xbg]))
)

birthweight_reg_test2<-cbind.data.frame(birthweight_reg_test2
                                         ,BWEIGHT_hat_xgb=predict(bw_xgb,newdata = as.matrix(birthweight_reg_test2[,quantitative_col_xbg]))
)

RMSE_xgb_train<-sqrt(mean((birthweight_reg_train2$BWEIGHT_hat_xgb*(y_max-y_min)+y_min-birthweight_reg_train2$BWEIGHT*(y_max-y_min)+y_min)^2))

RMSE_xgb_test<-sqrt(mean((birthweight_reg_test2$BWEIGHT_hat_xgb*(y_max-y_min)+y_min-birthweight_reg_test2$BWEIGHT*(y_max-y_min)+y_min)^2))

```
The RMSE of the normalised `BWEIGHT` is 0.076092, but the resulting non-normalised `BWEIGHT` is `r round(RMSE_xgb_test,4)`.

Since XGBoost can handle non-normal and non-linear dataset, normalisation is not necessary for the machine learning model. Would the model performance be any different if we ask XGBoost to fit the non-normalised `BWEIGHT`?

```{r max.height='180px'}

birthweight_reg_train2$BWEIGHT2<-birthweight_reg_train$BWEIGHT*(y_max-y_min)+y_min
birthweight_reg_test2$BWEIGHT2<-birthweight_reg_test$BWEIGHT*(y_max-y_min)+y_min

quantitative_col_xbg<-c(quantitative_col_reg[quantitative_col_reg!="BWEIGHT"],"SEX_dummy_Boy")

bw_xgb<-xgboost(data=as.matrix(birthweight_reg_train2[,quantitative_col_xbg])
                        ,label = birthweight_reg_train2$BWEIGHT2
                        , max.depth = 2, nrounds = 50)


birthweight_reg_train2<-cbind.data.frame(birthweight_reg_train2
                                         ,BWEIGHT_hat_xgb2=predict(bw_xgb,newdata = as.matrix(birthweight_reg_train2[,quantitative_col_xbg]))
)

birthweight_reg_test2<-cbind.data.frame(birthweight_reg_test2
                                         ,BWEIGHT_hat_xgb2=predict(bw_xgb,newdata = as.matrix(birthweight_reg_test2[,quantitative_col_xbg]))
)

RMSE_xgb_train2<-sqrt(mean((birthweight_reg_train2$BWEIGHT_hat_xgb2-birthweight_reg_train2$BWEIGHT2)^2))

RMSE_xgb_test2<-sqrt(mean((birthweight_reg_test2$BWEIGHT_hat_xgb2-birthweight_reg_test2$BWEIGHT2)^2))
```


### Evaluation of Regression Models

```{r max.height='180px'}

RMSE<-cbind.data.frame(`RMSE: Linear Regression`=c(RMSE_lm_train,RMSE_lm_test)
                       ,`RMSE: XGBoost`=c(RMSE_xgb_train,RMSE_xgb_test)
                       ,`RMSE: XGBoost (non-normalised)`=c(RMSE_xgb_train2,RMSE_xgb_test2)
  
)
rownames(RMSE)<-c("Train","Test")

RMSE

```

The improvement of RMSE between the two models are not significant. XGBoost performed much better on the non-normalised `BWEIGHT`. Usually normalisation would improve the model performance. However, normalisation leads to a worse model performance in this case study. This might be due to the normalisation approach used.

# Conclusion and Future works

We developed models with adequate performance for the classification of baby's birth weight type (i.e. underweight, normal, overweight) and the regression of baby's birth weight. Multinomial Logistic and kNN are used for predicting the birth weight type of babies. Multinomial Logistic outperform kNN by having a higher test sample accuracy of 0.857. Linear Regression and XGBoost are used to predict the birth weight of the babies. XGBoost on non-normalised birth weight outperforms other models by having a lower test sample RMSE of 0.9769. 

Several improvements can be made in the following areas:-

- **Pre-processing**: Explore other normalisation methods
- **Feature selection**: Select features via other methods, e.g. Recursive Feature Elimination (RFE) or stepwise regression
- **Model**: Experiment with other models such as neural network might perform better on this dataset
 

# Appendix: Metadata (trimmed datasets)

- SEX : Sex of the baby
- MARITAL : Marital status of its parents
- FAGE : Age of father
- GAINED : Weight gained during pregnancy
- VISITS : Number of prenatal visits
- MAGE : Age of mother
- FEDUC : Father's years of education
- MEDUC : Mother's years of education
- TOTALP : Total pregnancies
- BDEAD : number of children born alive now dead
- TERMS : Number of other terminations
- PLURAL : Whether the Mother gave birth to twins, triplets or quadruplets
- WEEKS : Completed weeks of gestation
- CIGNUM : Average number of cigarettes used daily (Mother)
- DRINKNUM: Average number of drinks used daily (mother)
- ANEMIA : Mother has/had anemia
- CARDIAC : Mother has/had cardiac disease
- ACLUNG : Mother has/had acute or chronic lung disease
- DIABETES : Mother has/had diabetes
- HERPES : Mother has/had genital herpes
- HYDRAM : Mother has/had hydramnios/Oligohydramnios
- HEMOGLOB : Mother has/had hemoglobinopathy
- HYPERCH : Mother has/had chronic hypertension
- HYPERPR : mother has/had pregnancy hypertension
- ECLAMP : Mother has/had Eclampsia
- CERVIX : Mother has/had incompetent cervix
- PINFANT : Mother had/had previous infant 4000+ grams
- PRETERM : Mother has/had previus preterm/small infant
- RENAL : Mother has/had renal disease
- RHSEN : Mother has/had Rh sensitization
- UTERINE : Mother has/had uterine bleeding
- BWEIGHT :  Baby's weight at birth

